{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919c0ed6-2fb6-419d-a7de-dc034fd5cc2b",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19140da9-7b42-4a59-861d-c10ee09748c2",
   "metadata": {},
   "source": [
    "Linear Regression: Linear regression is used for regression tasks, where the goal is to predict a continuous numerical output. It models the relationship between independent variables and a dependent variable by fitting a linear equation. For example, predicting house prices based on features like square footage and number of bedrooms.\n",
    "\n",
    "Logistic Regression: Logistic regression is used for classification tasks, where the goal is to predict a categorical outcome (usually binary, like yes/no or spam/not spam). It models the probability of an observation belonging to a particular class using a logistic (sigmoid) function. For example, predicting whether an email is spam or not based on email content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8d4ed-5ca7-407d-b263-b7005ed52495",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734aa5e-6736-488d-9a0e-729964df2b34",
   "metadata": {},
   "source": [
    "The cost function in logistic regression is typically the logistic loss or log loss (also known as cross-entropy loss). It quantifies the difference between predicted probabilities and actual labels. \n",
    "Optimization is typically done using gradient descent or its variants (e.g., stochastic gradient descent). The goal is to find the values of θ that minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cc371-ff8a-4b53-a508-c3a6f5f073cb",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bb514-529e-4dfc-8bd3-bbd5291a4559",
   "metadata": {},
   "source": [
    "Regularization in logistic regression adds a penalty term to the cost function to discourage large coefficient values (\n",
    "�\n",
    "θ). The two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "L1 regularization encourages sparse models by adding the absolute values of coefficients to the cost function. It can be used for feature selection.\n",
    "\n",
    "L2 regularization adds the squared values of coefficients to the cost function, which tends to shrink the coefficients toward zero. This helps prevent overfitting by reducing model complexity.\n",
    "\n",
    "The regularization parameter (\n",
    "�\n",
    "λ) controls the strength of regularization. A larger \n",
    "�\n",
    "λ results in stronger regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe170-281d-428e-ae3b-6eb8b66fdb45",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf85d1-e06b-4553-9365-078062f0d7ee",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of a binary classification model's performance across different thresholds. It plots the true positive rate (sensitivity) against the false positive rate (1-specificity) as the classification threshold varies.\n",
    "\n",
    "The area under the ROC curve (AUC-ROC) is a common metric used to quantify the overall performance of a logistic regression model. A model with an AUC-ROC of 0.5 is no better than random, while a model with an AUC-ROC of 1.0 is perfect.\n",
    "\n",
    "A higher AUC-ROC indicates better discrimination between positive and negative classes. The ROC curve helps visualize how well the model separates the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738829ba-90af-44e9-9f50-ab15d8a13016",
   "metadata": {},
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf9cb0-e474-4aac-b357-21ace7a17612",
   "metadata": {},
   "source": [
    "Forward Selection: Start with an empty set of features and iteratively add the most informative features one by one based on a chosen criterion (e.g., AIC, BIC). Helps prevent overfitting and simplifies the model.\n",
    "\n",
    "Backward Elimination: Start with all features and iteratively remove the least important features based on a chosen criterion. Similar to forward selection but may be more efficient for large feature sets.\n",
    "\n",
    "L1 Regularization (Lasso): Encourages sparsity in the model by shrinking some coefficients to exactly zero. Automatically selects important features.\n",
    "\n",
    "Recursive Feature Elimination (RFE): Ranks features by importance and recursively removes the least important ones until a desired number of features is reached. Works well with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aed597-9b76-483c-8ec6-a0fb7f2f1445",
   "metadata": {},
   "source": [
    "# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803deef-3f82-46c3-855f-3b5bd73d63fc",
   "metadata": {},
   "source": [
    "Resampling: You can oversample the minority class (adding more copies of minority samples) or undersample the majority class (removing some majority samples) to balance the dataset.\n",
    "\n",
    "Synthetic Data Generation: Techniques like Synthetic Minority Over-sampling Technique (SMOTE) generate synthetic samples for the minority class.\n",
    "\n",
    "**Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e676ff3-333e-4bd7-9398-51202e8f9a96",
   "metadata": {},
   "source": [
    "# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fe822-44fc-415b-93fe-4c5c1a05bd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
